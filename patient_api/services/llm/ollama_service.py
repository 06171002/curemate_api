import httpx
import json
import sys
from typing import Dict, Any, Optional
from patient_api.core.config import settings

from .base_llm_service import BaseLLMService, LLMConnectionError, LLMResponseError

# --- 1. Ollama ì„¤ì • (F-SUM-01 ì„¸ë¶€ì‚¬í•­) ---

# (ì„¤ì •) Ollama APIê°€ ì‹¤í–‰ ì¤‘ì¸ ì£¼ì†Œ
OLLAMA_API_URL = f"{settings.OLLAMA_BASE_URL}/api/generate"

# (ì„¤ì •) Ollamaì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "llama3", "gemma:7b")
OLLAMA_MODEL_NAME = settings.OLLAMA_MODEL_NAME

# API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (ì´ˆ). ìš”ì•½ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
API_TIMEOUT = settings.OLLAMA_TIMEOUT

# httpx í´ë¼ì´ì–¸íŠ¸ëŠ” ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
# ë¹„ë™ê¸°(async) í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (ì›Œì»¤ê°€ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œí•  ê²ƒì„ ëŒ€ë¹„)
_client = httpx.AsyncClient(timeout=API_TIMEOUT)


# --- í”„ë¡¬í”„íŠ¸ ìƒì„± (F-SUM-02) ---

def _build_summary_prompt(transcript_text: str) -> str:
    """
    ëª…ì„¸ì„œ F-SUM-02ì— ì •ì˜ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ìš”ì•½ í’ˆì§ˆì„ ë†’ì´ë ¤ë©´ ì´ í•¨ìˆ˜ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤.
    """
    # { } ì•ˆì— ë³€ìˆ˜ë¥¼ ë„£ê¸° ìœ„í•´ f-stringì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # JSON í˜•ì‹ ìì²´ë„ { }ë¥¼ í¬í•¨í•˜ë¯€ë¡œ,
    # JSONì˜ { }ëŠ” {{ }} (ë‘ ë²ˆ)ìœ¼ë¡œ ê°ì‹¸ì„œ f-stringì´ ë³€ìˆ˜ë¡œ ì˜¤í•´í•˜ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤.

    # prompt = f"""
    #                 ë‹¹ì‹ ì€ ì˜ì‚¬ì™€ í™˜ìì˜ ëŒ€í™”ë¡ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì˜ë£Œ ë¹„ì„œì…ë‹ˆë‹¤.
    #                 ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ì˜ JSON í˜•ì‹ì— ë§ì¶° í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.
    #
    #                 [ëŒ€í™”ë¡]
    #                 {transcript_text}
    #
    #                 [ìš”ì•½ í˜•ì‹ (JSON)]
    #                 {{
    #                   "main_complaint": "í™˜ìê°€ í˜¸ì†Œí•˜ëŠ” ì£¼ìš” ì¦ìƒ (CC)",
    #                   "diagnosis": "ì˜ì‚¬ì˜ ì†Œê²¬ ë° ì§„ë‹¨ëª…",
    #                   "recommendation": "ì²˜ë°©, ê²€ì‚¬ ê³„íš, ë˜ëŠ” ìƒí™œ ê¶Œê³  ì‚¬í•­"
    #                 }}
    #
    #                 [ì§€ì¹¨]
    #                 * ëŒ€í™”ë¡ì— ì •ë³´ê°€ ë¶€ì¡±í•œ í•­ëª©ì€ "ì •ë³´ ì—†ìŒ" ë˜ëŠ” "ì–¸ê¸‰ë˜ì§€ ì•ŠìŒ"ìœ¼ë¡œ ì±„ìš°ì„¸ìš”.
    #                 * [ëŒ€í™”ë¡] ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ **ì‚¬ì‹¤(Fact)**ë§Œì„ ìš”ì•½í•˜ì„¸ìš”.
    #                 * ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ê·¸ ì™¸ì˜ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    #         """
    prompt = f"""
                        ì•„ë˜ ë¬¸ì¥ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
                        {transcript_text}
                """
    return prompt


class OllamaService(BaseLLMService):
    """Ollama LLM ì„œë¹„ìŠ¤ êµ¬í˜„"""

    def __init__(self):
        self.api_url = OLLAMA_API_URL
        self.model_name = OLLAMA_MODEL_NAME
        self.client = httpx.AsyncClient(timeout=API_TIMEOUT)

    async def check_connection(self) -> bool:
        try:
            print("[Ollama Service] ğŸŸ¡ ì„œë²„ ì—°ê²° í™•ì¸...")
            await self.client.get("http://host.docker.internal:11434/api/tags")
            print(f"[Ollama Service] ğŸŸ¢ ì—°ê²° ì„±ê³µ (ëª¨ë¸: {self.model_name})")
            return True
        except httpx.RequestError as e:
            print(f"[Ollama Service] ğŸ”´ ì—°ê²° ì‹¤íŒ¨: {e}", file=sys.stderr)
            raise LLMConnectionError(f"Ollama ì—°ê²° ì‹¤íŒ¨: {e}")

    async def get_summary(transcript: str) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ëŒ€ë³¸ì„ ë°›ì•„ Ollamaì— ìš”ì•½ì„ ìš”ì²­í•˜ê³ ,
        íŒŒì‹±ëœ ë”•ì…”ë„ˆë¦¬(JSON)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        print(f"[Ollama Service] ğŸ”µ ìš”ì•½ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        # 1. í”„ë¡¬í”„íŠ¸ ìƒì„± (F-SUM-02)
        prompt_text = _build_summary_prompt(transcript)

        # 2. API í˜¸ì¶œ (F-SUM-01)
        payload = {
            "model": OLLAMA_MODEL_NAME,
            "prompt": prompt_text,
            "stream": False,
            "format": "json"  # â˜…â˜…â˜… Ollamaì— JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ë„ë¡ ê°•ì œ (íŒŒì‹±ì´ ì‰¬ì›Œì§)
        }

        raw_response_string = None
        try:
            response = await _client.post(OLLAMA_API_URL, json=payload)

            # 4xx, 5xx ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚´
            response.raise_for_status()

            # Ollamaê°€ 'format: json'ì„ ì‚¬ìš©í•˜ë©´ 'response' í‚¤ì— JSON "ë¬¸ìì—´"ì„ ë‹´ì•„ì¤ë‹ˆë‹¤.
            raw_response_string = response.json()["response"]

        except httpx.HTTPStatusError as e:
            print(f"[Ollama Service] ğŸ”´ Ollama API ì˜¤ë¥˜ (HTTP {e.response.status_code}): {e.response.text}",
                  file=sys.stderr)
            raise RuntimeError(f"Ollama API ì˜¤ë¥˜: {e.response.text}")
        except httpx.RequestError as e:
            print(f"[Ollama Service] ğŸ”´ Ollama ì—°ê²° ì˜¤ë¥˜: {e}", file=sys.stderr)
            raise RuntimeError(f"Ollama ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            print(f"[Ollama Service] ğŸ”´ ìš”ì•½ ìš”ì²­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}", file=sys.stderr)
            raise e  # worker.pyê°€ ì²˜ë¦¬í•˜ë„ë¡ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´

        # 3. ê²°ê³¼ íŒŒì‹± (F-SUM-03)
        if not raw_response_string:
            print("[Ollama Service] ğŸ”´ Ollamaê°€ ë¹„ì–´ìˆëŠ” ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
            raise ValueError("Ollamaê°€ ë¹„ì–´ìˆëŠ” ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")

        try:
            # 'format: json' ì˜µì…˜ ë•ë¶„ì— raw_response_string ìì²´ê°€
            # ê¹¨ë—í•œ JSON ë¬¸ìì—´ì…ë‹ˆë‹¤.
            # (ì˜ˆ: '{\n  "main_complaint": "ë³µí†µ"\n}')

            summary_dict = json.loads(raw_response_string)

            print(f"[Ollama Service] ğŸŸ¢ ìš”ì•½ ì‘ì—… ì™„ë£Œ ë° JSON íŒŒì‹± ì„±ê³µ.")
            return summary_dict

        except json.JSONDecodeError:
            print(f"[Ollama Service] ğŸ”´ Ollama ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨!", file=sys.stderr)
            print(f"[Ollama Service] ğŸ”´ Ollama ì›ë³¸ ì‘ë‹µ: {raw_response_string}", file=sys.stderr)
            raise ValueError("Ollamaê°€ ë°˜í™˜í•œ ìš”ì•½ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

    async def get_medical_summary(self, transcript: str) -> Dict[str, Any]:
        # ... (ê¸°ì¡´ ë¡œì§ ë˜ëŠ” get_summary í˜¸ì¶œ)
        pass


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ollama_service = OllamaService()