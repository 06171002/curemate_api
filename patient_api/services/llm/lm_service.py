# patient_api/services/llm_service.py

import json
import sys
from typing import Dict, Any
from openai import AsyncOpenAI
import httpx
import re
from .base_llm_service import BaseLLMService, LLMConnectionError, LLMResponseError
from patient_api.core.config import settings

# --- 1. LM Studio ì„¤ì • ---
LMSTUDIO_BASE_URL = settings.LMSTUDIO_BASE_URL
LMSTUDIO_HEALTH_URL = settings.LMSTUDIO_BASE_URL.replace("/v1", "")

_client = AsyncOpenAI(
    base_url=LMSTUDIO_BASE_URL,
    api_key="lm-studio",
    timeout=settings.LMSTUDIO_TIMEOUT
)


async def check_llm_connection():
    """
    FastAPI ì„œë²„ ì‹œì‘ ì‹œ LM Studio ì„œë²„ê°€ ì¼œì ¸ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        print("[LLM Service] ğŸŸ¡ LM Studio ì„œë²„ ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        async with httpx.AsyncClient() as client:
            response = await client.get(LMSTUDIO_HEALTH_URL)
            response.raise_for_status()
        print(f"[LLM Service] ğŸŸ¢ LM Studio ì„œë²„ ì—°ê²° ì„±ê³µ.")
    except httpx.RequestError as e:
        print(f"[LLM Service] ğŸ”´ LM Studio ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}", file=sys.stderr)
        print("[LLM Service] ğŸ”´ LM Studioê°€ Windowsì—ì„œ 0.0.0.0 í˜¸ìŠ¤íŠ¸ë¡œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)
    except Exception as e:
        print(f"[LLM Service] ğŸ”´ LM Studio ì—°ê²° ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}", file=sys.stderr)


# --- 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ---

def _build_simple_summary_prompt(transcript_text: str) -> str:
    """
    ê°„ë‹¨í•œ ìš”ì•½ í”„ë¡¬í”„íŠ¸ (í…ŒìŠ¤íŠ¸ìš©)
    """
    prompt = f"""
ì•„ë˜ ë¬¸ì¥ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

[í…ìŠ¤íŠ¸]
{transcript_text}

[ìš”ì•½ í˜•ì‹ (JSON)]
{{
  "summary": "ìš”ì•½ëœ ë‚´ìš©"
}}

[ì§€ì¹¨]
* JSON í˜•ì‹ë§Œ ì‘ë‹µìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
* ë§ˆí¬ë‹¤ìš´, ì½”ë“œ ë¸”ë¡, ì¶”ê°€ ì„¤ëª… ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""
    return prompt


def _build_medical_summary_prompt(transcript_text: str) -> str:
    """
    ì˜ë£Œ ëŒ€í™” ìš”ì•½ í”„ë¡¬í”„íŠ¸ (í”„ë¡œë•ì…˜ìš©)
    """
    prompt = f"""
ë‹¹ì‹ ì€ ì˜ì‚¬ì™€ í™˜ìì˜ ëŒ€í™”ë¡ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì˜ë£Œ ë¹„ì„œì…ë‹ˆë‹¤.
ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.

[ëŒ€í™”ë¡]
{transcript_text}

[ìš”ì•½ í˜•ì‹ (JSON)]
{{
  "main_complaint": "í™˜ìê°€ í˜¸ì†Œí•˜ëŠ” ì£¼ìš” ì¦ìƒ",
  "diagnosis": "ì˜ì‚¬ì˜ ì†Œê²¬ ë° ì§„ë‹¨ëª…",
  "recommendation": "ì²˜ë°©, ê²€ì‚¬ ê³„íš, ë˜ëŠ” ìƒí™œ ê¶Œê³  ì‚¬í•­"
}}

[ì§€ì¹¨]
* JSON í˜•ì‹ë§Œ ì‘ë‹µìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
* ë§ˆí¬ë‹¤ìš´, ì½”ë“œ ë¸”ë¡, ì¶”ê°€ ì„¤ëª… ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
* ê° í•­ëª©ì€ ê°„ê²°í•˜ê²Œ 1-2ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ì‘ë‹µ ì˜ˆì‹œ]
{{
  "main_complaint": "3ì¼ ì „ë¶€í„° ì§€ì†ë˜ëŠ” ë‘í†µê³¼ ì–´ì§€ëŸ¬ì›€",
  "diagnosis": "í¸ë‘í†µ ì˜ì‹¬, í˜ˆì•• ì •ìƒ ë²”ìœ„",
  "recommendation": "íƒ€ì´ë ˆë†€ ë³µìš©, ì¶©ë¶„í•œ íœ´ì‹, 1ì£¼ì¼ í›„ ì¬ë°©ë¬¸"
}}
"""
    return prompt


def _build_structured_extraction_prompt(transcript_text: str) -> str:
    """
    êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ (ìƒì„¸ ë²„ì „)
    """
    prompt = f"""
ë‹¹ì‹ ì€ ì˜ë£Œ ëŒ€í™”ë¡ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤.
ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.

[ëŒ€í™”ë¡]
{transcript_text}

[ì¶”ì¶œ í˜•ì‹ (JSON)]
{{
  "patient_info": {{
    "age": "í™˜ì ë‚˜ì´ (ì–¸ê¸‰ë˜ì§€ ì•Šìœ¼ë©´ null)",
    "gender": "í™˜ì ì„±ë³„ (ì–¸ê¸‰ë˜ì§€ ì•Šìœ¼ë©´ null)"
  }},
  "symptoms": [
    "ì¦ìƒ1",
    "ì¦ìƒ2"
  ],
  "duration": "ì¦ìƒ ì§€ì† ê¸°ê°„",
  "severity": "ì¦ìƒ ì‹¬ê°ë„ (ê²½ì¦/ì¤‘ë“±ë„/ì¤‘ì¦)",
  "diagnosis": "ì˜ì‚¬ì˜ ì§„ë‹¨ëª…",
  "prescription": [
    "ì²˜ë°© ì•½ë¬¼1",
    "ì²˜ë°© ì•½ë¬¼2"
  ],
  "tests_ordered": [
    "ê²€ì‚¬ í•­ëª©1"
  ],
  "follow_up": "ì¶”í›„ ê´€ë¦¬ ê³„íš",
  "lifestyle_advice": "ìƒí™œ ìŠµê´€ ê¶Œê³  ì‚¬í•­"
}}

[ì§€ì¹¨]
* JSON í˜•ì‹ë§Œ ì‘ë‹µìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
* ì–¸ê¸‰ë˜ì§€ ì•Šì€ í•­ëª©ì€ null ë˜ëŠ” ë¹ˆ ë°°ì—´([])ë¡œ í‘œì‹œí•˜ì„¸ìš”.
* ë°°ì—´ í•­ëª©ì€ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
"""
    return prompt


# --- 3. JSON íŒŒì‹± ìœ í‹¸ë¦¬í‹° ---

def _parse_json_response(raw_response: str) -> Dict[str, Any]:
    """
    LM Studio ì‘ë‹µì—ì„œ JSONì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•©ë‹ˆë‹¤.
    """
    if not raw_response:
        raise ValueError("LM Studioê°€ ë¹„ì–´ìˆëŠ” ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")

    try:
        # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        cleaned = re.sub(r'```json\s*|\s*```', '', raw_response)
        cleaned = re.sub(r'"""\s*|\s*"""', '', cleaned)
        cleaned = cleaned.strip()

        # 2. ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ë§Œ ì¶”ì¶œ
        match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', cleaned, re.DOTALL)

        if not match:
            print(f"[LLM Service] ğŸ”´ JSON íŒŒì‹± ì‹¤íŒ¨: ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
            print(f"[LLM Service] ğŸ”´ ì›ë³¸ ì‘ë‹µ:\n{raw_response}", file=sys.stderr)
            raise ValueError("ì‘ë‹µì—ì„œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        json_string = match.group(0).strip()

        # 3. JSON íŒŒì‹±
        parsed_data = json.loads(json_string)

        print(f"[LLM Service] ğŸŸ¢ JSON íŒŒì‹± ì„±ê³µ: {list(parsed_data.keys())}")
        return parsed_data

    except json.JSONDecodeError as e:
        print(f"[LLM Service] ğŸ”´ JSON ë””ì½”ë”© ì‹¤íŒ¨! (Error: {e})", file=sys.stderr)
        print(f"[LLM Service] ğŸ”´ ì¶”ì¶œëœ JSON ë¬¸ìì—´:\n{json_string if 'json_string' in locals() else 'N/A'}", file=sys.stderr)
        print(f"[LLM Service] ğŸ”´ ì›ë³¸ ì‘ë‹µ (ì „ì²´):\n{raw_response}", file=sys.stderr)
        raise ValueError("LM Studioê°€ ë°˜í™˜í•œ ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")


# --- 4. LLM í˜¸ì¶œ ë˜í¼ ---

async def _call_llm(prompt: str, temperature: float = 0.0) -> str:
    """
    LM Studio APIë¥¼ í˜¸ì¶œí•˜ëŠ” ê³µí†µ í•¨ìˆ˜
    """
    try:
        response = await _client.chat.completions.create(
            model="local-model",
            messages=[
                {"role": "system", "content": prompt}
            ],
            temperature=temperature,
        )

        raw_response = response.choices[0].message.content
        return raw_response

    except httpx.RequestError as e:
        print(f"[LLM Service] ğŸ”´ LM Studio ì—°ê²° ì˜¤ë¥˜: {e}", file=sys.stderr)
        raise RuntimeError(f"LM Studio ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        print(f"[LLM Service] ğŸ”´ LLM í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}", file=sys.stderr)
        raise e


# --- 5. ê³µê°œ API í•¨ìˆ˜ë“¤ ---

async def get_simple_summary(transcript: str) -> Dict[str, Any]:
    """
    ê°„ë‹¨í•œ ìš”ì•½ (í…ŒìŠ¤íŠ¸ìš©)

    ë°˜í™˜ í˜•ì‹:
    {
        "summary": "ìš”ì•½ëœ ë‚´ìš©"
    }
    """
    print(f"[LLM Service] ğŸ”µ ê°„ë‹¨ ìš”ì•½ ì‘ì—… ì‹œì‘...")

    prompt = _build_simple_summary_prompt(transcript)
    raw_response = await _call_llm(prompt, temperature=0.0)
    summary_dict = _parse_json_response(raw_response)

    # í•„ìˆ˜ í‚¤ ê²€ì¦
    if "summary" not in summary_dict:
        raise ValueError("ì‘ë‹µì— 'summary' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print(f"[LLM Service] ğŸŸ¢ ê°„ë‹¨ ìš”ì•½ ì™„ë£Œ")
    return summary_dict


async def get_medical_summary(transcript: str) -> Dict[str, Any]:
    """
    ì˜ë£Œ ëŒ€í™” ìš”ì•½ (í”„ë¡œë•ì…˜ìš©)

    ë°˜í™˜ í˜•ì‹:
    {
        "main_complaint": "í™˜ìê°€ í˜¸ì†Œí•˜ëŠ” ì£¼ìš” ì¦ìƒ",
        "diagnosis": "ì˜ì‚¬ì˜ ì†Œê²¬ ë° ì§„ë‹¨ëª…",
        "recommendation": "ì²˜ë°©, ê²€ì‚¬ ê³„íš, ë˜ëŠ” ìƒí™œ ê¶Œê³  ì‚¬í•­"
    }
    """
    print(f"[LLM Service] ğŸ”µ ì˜ë£Œ ìš”ì•½ ì‘ì—… ì‹œì‘...")

    prompt = _build_medical_summary_prompt(transcript)
    raw_response = await _call_llm(prompt, temperature=0.0)
    summary_dict = _parse_json_response(raw_response)

    # í•„ìˆ˜ í‚¤ ê²€ì¦
    required_keys = ["main_complaint", "diagnosis", "recommendation"]
    missing_keys = [key for key in required_keys if key not in summary_dict]

    if missing_keys:
        print(f"[LLM Service] âš ï¸ ì‘ë‹µì— ëˆ„ë½ëœ í‚¤: {missing_keys}", file=sys.stderr)
        # ëˆ„ë½ëœ í‚¤ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€ (ìœ ì—°í•œ ì²˜ë¦¬)
        for key in missing_keys:
            summary_dict[key] = ""

    print(f"[LLM Service] ğŸŸ¢ ì˜ë£Œ ìš”ì•½ ì™„ë£Œ")
    return summary_dict


async def get_structured_data(transcript: str) -> Dict[str, Any]:
    """
    êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ (ìƒì„¸ ë²„ì „)

    ë°˜í™˜ í˜•ì‹:
    {
        "patient_info": {"age": "...", "gender": "..."},
        "symptoms": ["ì¦ìƒ1", "ì¦ìƒ2"],
        "duration": "...",
        "severity": "...",
        "diagnosis": "...",
        "prescription": ["ì•½ë¬¼1"],
        "tests_ordered": ["ê²€ì‚¬1"],
        "follow_up": "...",
        "lifestyle_advice": "..."
    }
    """
    print(f"[LLM Service] ğŸ”µ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì‘ì—… ì‹œì‘...")

    prompt = _build_structured_extraction_prompt(transcript)
    raw_response = await _call_llm(prompt, temperature=0.0)
    structured_dict = _parse_json_response(raw_response)

    print(f"[LLM Service] ğŸŸ¢ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
    return structured_dict


# --- 6. (ë ˆê±°ì‹œ) ê¸°ì¡´ í•¨ìˆ˜ ì´ë¦„ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±) ---

async def get_summary(transcript: str) -> Dict[str, Any]:
    """
    ê¸°ë³¸ ìš”ì•½ í•¨ìˆ˜ (í˜„ì¬ëŠ” ê°„ë‹¨ ìš”ì•½ ì‚¬ìš©)

    ë‚˜ì¤‘ì— get_medical_summary()ë¡œ ë³€ê²½ ê°€ëŠ¥
    """
    return await get_simple_summary(transcript)


class LmService(BaseLLMService):
    """LM Studio ì„œë¹„ìŠ¤ êµ¬í˜„"""

    def __init__(self):
        self.base_url = LMSTUDIO_BASE_URL
        self.health_url = LMSTUDIO_HEALTH_URL
        self.client = AsyncOpenAI(base_url=self.base_url, api_key="lm-studio")

    async def check_connection(self) -> bool:
        try:
            print("[LM Studio Service] ğŸŸ¡ ì„œë²„ ì—°ê²° í™•ì¸...")
            async with httpx.AsyncClient() as client:
                response = await client.get(self.health_url)
                response.raise_for_status()
            print(f"[LM Studio Service] ğŸŸ¢ ì—°ê²° ì„±ê³µ")
            return True
        except httpx.RequestError as e:
            print(f"[LM Studio Service] ğŸ”´ ì—°ê²° ì‹¤íŒ¨: {e}")
            raise LLMConnectionError(f"LM Studio ì—°ê²° ì‹¤íŒ¨: {e}")

    async def get_summary(self, transcript: str) -> Dict[str, Any]:
        return await get_simple_summary(transcript)

    async def get_medical_summary(self, transcript: str) -> Dict[str, Any]:
        return await get_medical_summary(transcript)


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
lm_service = LmService()