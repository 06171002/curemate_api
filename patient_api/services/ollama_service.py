# patient_api/services/llm_service.py

import json
import sys
from typing import Dict, Any
from openai import OpenAI, AsyncOpenAI
import httpx

# --- 1. LM Studio ì„¤ì • ---
# (LM StudioëŠ” 1234 í¬íŠ¸ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©)
LMSTUDIO_BASE_URL = "http://host.docker.internal:1234/v1"
LMSTUDIO_HEALTH_URL = "http://host.docker.internal:1234"

# (â˜…ìˆ˜ì •) OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
_client = AsyncOpenAI(
    base_url=LMSTUDIO_BASE_URL,
    api_key="lm-studio"  # (LM StudioëŠ” API í‚¤ê°€ í•„ìš” ì—†ì§€ë§Œ, í˜•ì‹ìƒ ì•„ë¬´ ê°’ì´ë‚˜ ì…ë ¥)
)


async def check_llm_connection():
    """
    FastAPI ì„œë²„ ì‹œì‘ ì‹œ LM Studio ì„œë²„ê°€ ì¼œì ¸ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        print("[LLM Service] ğŸŸ¡ LM Studio ì„œë²„ ì—°ê²°ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        async with httpx.AsyncClient() as client:
            response = await client.get(LMSTUDIO_HEALTH_URL)
            response.raise_for_status()
        print(f"[LLM Service] ğŸŸ¢ LM Studio ì„œë²„ ì—°ê²° ì„±ê³µ.")
    except httpx.RequestError as e:
        print(f"[LLM Service] ğŸ”´ LM Studio ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}", file=sys.stderr)
        print("[LLM Service] ğŸ”´ LM Studioê°€ Windowsì—ì„œ 0.0.0.0 í˜¸ìŠ¤íŠ¸ë¡œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)
    except Exception as e:
        print(f"[LLM Service] ğŸ”´ LM Studio ì—°ê²° ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}", file=sys.stderr)


# --- 2. í”„ë¡¬í”„íŠ¸ ìƒì„± ---
def _build_summary_prompt(transcript_text: str) -> str:
    """
    (â˜…ìˆ˜ì •) Ollama í”„ë¡¬í”„íŠ¸ë¥¼ JSON êµ¬ì¡°í™” í”„ë¡¬í”„íŠ¸ë¡œ ë³€ê²½
    """
    prompt = f"""
            ë‹¹ì‹ ì€ ì˜ì‚¬ì™€ í™˜ìì˜ ëŒ€í™”ë¡ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì˜ë£Œ ë¹„ì„œì…ë‹ˆë‹¤.
            ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ì˜ JSON í˜•ì‹ì— ë§ì¶° í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.
            [ëŒ€í™”ë¡]
            {transcript_text}
            [ìš”ì•½ í˜•ì‹ (JSON)]
            {{
              "main_complaint": "í™˜ìê°€ í˜¸ì†Œí•˜ëŠ” ì£¼ìš” ì¦ìƒ (CC)",
              "diagnosis": "ì˜ì‚¬ì˜ ì†Œê²¬ ë° ì§„ë‹¨ëª…",
              "recommendation": "ì²˜ë°©, ê²€ì‚¬ ê³„íš, ë˜ëŠ” ìƒí™œ ê¶Œê³  ì‚¬í•­"
            }}
            [ì§€ì¹¨]
            * JSON í˜•ì‹ë§Œ ì‘ë‹µìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    """
    return prompt


# --- 3. í•µì‹¬ ê¸°ëŠ¥: ìš”ì•½ ìš”ì²­ í•¨ìˆ˜ ---
async def get_summary(transcript: str) -> Dict[str, Any]:
    """
    (â˜…ìˆ˜ì •) OpenAI í˜¸í™˜ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ì„ ìš”ì²­í•©ë‹ˆë‹¤.
    """
    print(f"[LLM Service] ğŸ”µ ìš”ì•½ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    system_prompt = _build_summary_prompt(transcript)

    try:
        # (â˜…ìˆ˜ì •) OpenAI API í˜•ì‹ìœ¼ë¡œ í˜¸ì¶œ
        response = await _client.chat.completions.create(
            model="local-model",  # (LM Studioì—ì„œëŠ” ì´ ê°’ì´ ë¬´ì‹œë¨)
            messages=[
                {"role": "system", "content": system_prompt}
            ],
            temperature=0.0,
        )

        raw_response_string = response.choices[0].message.content

    except httpx.RequestError as e:
        print(f"[LLM Service] ğŸ”´ LM Studio ì—°ê²° ì˜¤ë¥˜: {e}", file=sys.stderr)
        raise RuntimeError(f"LM Studio ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    except Exception as e:
        print(f"[LLM Service] ğŸ”´ ìš”ì•½ ìš”ì²­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}", file=sys.stderr)
        raise e

    # ... (ì´í•˜ json.loads()ë¥¼ ì‚¬ìš©í•œ íŒŒì‹± ë¡œì§ì€ ë™ì¼)
    if not raw_response_string:
        # ...
        raise ValueError("LM Studioê°€ ë¹„ì–´ìˆëŠ” ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")

    try:
        summary_dict = json.loads(raw_response_string)
        print(f"[LLM Service] ğŸŸ¢ ìš”ì•½ ì‘ì—… ì™„ë£Œ ë° JSON íŒŒì‹± ì„±ê³µ.")
        return summary_dict
    except json.JSONDecodeError:
        # ...
        raise ValueError("LM Studioê°€ ë°˜í™˜í•œ ìš”ì•½ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")