# STT/Summary API

μ‹¤μ‹κ°„ STT(Speech-to-Text) λ° μ”μ•½ API μ„λ²„μ…λ‹λ‹¤.

μ΄ ν”„λ΅μ νΈλ” `FastAPI`, `Celery`, `Redis`, `faster-whisper`λ¥Ό Docker Composeλ΅ μ‹¤ν–‰ν•κ³ , `Ollama`λ” λ΅μ»¬ νΈμ¤νΈ(Host) PCμ—μ„ μ‹¤ν–‰ν•©λ‹λ‹¤.

---

## π€ μ‹¤ν–‰ λ°©λ²•

### 1. μ‚¬μ „ μ¤€λΉ„ (μ΄ 3κ°€μ§€)

1.  **Docker Desktop**μ„ μ„¤μΉν•κ³  μ‹¤ν–‰ν•΄μ•Ό ν•©λ‹λ‹¤.
2.  **Ollama**λ¥Ό **νΈμ¤νΈ PC(Windows/Mac)μ— μ§μ ‘ μ„¤μΉ**ν•΄μ•Ό ν•©λ‹λ‹¤.
3.  Ollamaμ—μ„ μ‚¬μ©ν•  λ¨λΈ(`gemma3`)μ„ λ―Έλ¦¬ λ°›μ•„μ•Ό ν•©λ‹λ‹¤.
    ```bash
    ollama pull gemma3
    ```

### 2. ν”„λ΅μ νΈ ν΄λ΅ 

### 3. μ‹¤ν–‰

1.  **[ν„°λ―Έλ„ 1]** λ΅μ»¬ PC(Windows)μ—μ„ `Ollama` μ„λ²„λ¥Ό **0.0.0.0 νΈμ¤νΈ**λ΅ μ‹¤ν–‰ (`$env:OLLAMA_HOST="0.0.0.0"`, `ollama serve`)ν•κ³  λ°©ν™”λ²½μ„ ν—μ©ν•©λ‹λ‹¤.
2.  **[ν„°λ―Έλ„ 2]** `docker-compose up -d --build`λ¥Ό μ‹¤ν–‰ν•©λ‹λ‹¤.

----------------------------

## π“‚ ν”„λ΅μ νΈ κµ¬μ΅°

<details>

```bash
stt_api/
β”β”€β”€ __init__.py                    # ν¨ν‚¤μ§€ μ΄κΈ°ν™”
β”β”€β”€ main.py                        # FastAPI μ• ν”λ¦¬μΌ€μ΄μ… μ§„μ…μ  (μ„λ²„ μ‹¤ν–‰, λΌμ°ν„° λ“±λ΅)
β”‚
β”β”€β”€ core/                          # π”§ ν•µμ‹¬ μ„¤μ • λ° μΈν”„λΌ
β”‚   β”β”€β”€ __init__.py
β”‚   β”β”€β”€ config.py                  # ν™κ²½ μ„¤μ • κ΄€λ¦¬ (Settings, Constants, active_jobs)
β”‚   β”β”€β”€ celery_config.py           # Celery μ‘μ—… ν μ„¤μ • (λΈλ΅μ»¤, μ›μ»¤)
β”‚   β”β”€β”€ logging_config.py          # κµ¬μ΅°ν™”λ λ΅κΉ… μ‹μ¤ν… (JSON/μ»¬λ¬ ν¬λ§·, StructuredLogger)
β”‚   β””β”€β”€ exceptions.py              # μ»¤μ¤ν…€ μμ™Έ μ •μ (CustomException, λ„λ©”μΈλ³„ μμ™Έ)
β”‚
β”β”€β”€ domain/                        # π“¦ λ„λ©”μΈ λ¨λΈ (λΉ„μ¦λ‹μ¤ μ—”ν‹°ν‹°)
β”‚   β”β”€β”€ __init__.py
β”‚   β””β”€β”€ streaming_job.py           # μ‹¤μ‹κ°„ μ¤νΈλ¦Ό μ‘μ—… λ¨λΈ (VAD, λ€ν™”λ΅ κ΄€λ¦¬)
β”‚
β”β”€β”€ services/                      # π› οΈ λΉ„μ¦λ‹μ¤ λ΅μ§ μ„λΉ„μ¤
β”‚   β”β”€β”€ __init__.py
β”‚   β”‚
β”‚   β”β”€β”€ stt/                       # π¤ μμ„±-ν…μ¤νΈ λ³€ν™ (STT)
β”‚   β”‚   β”β”€β”€ __init__.py            # μ„λΉ„μ¤ μ§„μ…μ  (ν•¨μ λ…Έμ¶)
β”‚   β”‚   β”β”€β”€ whisper_service.py     # Whisper λ¨λΈ STT (λ°°μΉ/μ¤νΈλ¦¬λ° λ³€ν™)
β”‚   β”‚   β””β”€β”€ vad_processor.py       # μμ„± ν™λ™ κ°μ§€ (VAD, μ‹¤μ‹κ°„ μ„Έκ·Έλ¨ΌνΈ λ¶„λ¦¬)
β”‚   β”‚
β”‚   β”β”€β”€ llm/                       # π¤– LLM μ”μ•½ μ„λΉ„μ¤
β”‚   β”‚   β”β”€β”€ __init__.py            # ν”„λ΅λ°”μ΄λ” μλ™ μ„ νƒ (Ollama/LM Studio)
β”‚   β”‚   β”β”€β”€ base_llm_service.py    # LLM μ„λΉ„μ¤ μ¶”μƒ ν΄λμ¤ (μΈν„°νμ΄μ¤ μ •μ)
β”‚   β”‚   β”β”€β”€ ollama_service.py      # Ollama LLM κµ¬ν„μ²΄ (λ΅μ»¬ LLM)
β”‚   β”‚   β””β”€β”€ lm_service.py          # LM Studio κµ¬ν„μ²΄ (OpenAI νΈν™ API)
β”‚   β”‚
β”‚   β”β”€β”€ storage/                   # π’Ύ λ°μ΄ν„° μ €μ¥ λ° μ‘μ—… κ΄€λ¦¬
β”‚   β”‚   β”β”€β”€ __init__.py            # μ„λΉ„μ¤ ν†µν•© μ§„μ…μ 
β”‚   β”‚   β”β”€β”€ job_manager.py         # μ‘μ—… μƒλ…μ£ΌκΈ° ν†µν•© κ΄€λ¦¬ (DB + Redis μ΅°μ¨)
β”‚   β”‚   β”β”€β”€ database_service.py    # DB μ¶”μƒν™” λ μ΄μ–΄ (PostgreSQL/MySQL λ€λΉ„)
β”‚   β”‚   β””β”€β”€ cache_service.py       # Redis μΊμ‹ μ„λΉ„μ¤ (λΉ λ¥Έ μ΅°ν, Pub/Sub)
β”‚   β”‚
β”‚   β”β”€β”€ pipeline/                  # π”„ μ›ν¬ν”λ΅μ° νμ΄ν”„λΌμΈ
β”‚   β”‚   β”β”€β”€ __init__.py
β”‚   β”‚   β”β”€β”€ batch_pipeline.py      # λ°°μΉ μ²λ¦¬ νμ΄ν”„λΌμΈ (νμΌ μ—…λ΅λ“ β†’ STT β†’ μ”μ•½)
β”‚   β”‚   β””β”€β”€ stream_pipeline.py     # μ‹¤μ‹κ°„ μ¤νΈλ¦¬λ° νμ΄ν”„λΌμΈ (WebSocket β†’ VAD β†’ STT β†’ μ”μ•½)
β”‚   β”‚
β”‚   β””β”€β”€ tasks.py                   # β™οΈ Celery λ°±κ·ΈλΌμ΄λ“ μ‘μ—… (λΉ„λ™κΈ° νμ΄ν”„λΌμΈ μ‹¤ν–‰)
β”‚
β””β”€β”€ api/                           # π FastAPI μ—”λ“ν¬μΈνΈ
    β”β”€β”€ __init__.py
    β”β”€β”€ batch_endpoints.py         # λ°°μΉ μ‘μ—… API (POST /request, GET /result, SSE /stream-events)
    β””β”€β”€ stream_endpoints.py        # μ‹¤μ‹κ°„ μ¤νΈλ¦Ό API (POST /create, WebSocket /ws)
```
</details>

---------------------

## π’Ύ νμΌ ν›„μ²λ¦¬ (SSE) μ•„ν‚¤ν…μ² νλ¦„

`POST /api/v1/conversation/request` (νμΌ μ—…λ΅λ“)μ™€ `GET /api/v1/conversation/stream-events/{job_id}` (SSE μ¤νΈλ¦Ό) μ”μ²­ μ‹μ μƒμ„Έ νλ¦„μ…λ‹λ‹¤.

```mermaid
sequenceDiagram
    participant C as Client (App)
    participant API as API Server
    participant R as Redis (Pub/Sub)
    participant W as Celery Worker
    participant L as LLM (Ollama)

    Note over C, API: 1λ‹¨κ³„: μ‘μ—… μ”μ²­
    C->>API: POST /request (Audio File)
    API->>R: Create Job (Pending)
    API->>W: Task Queueing (Celery)
    API-->>C: Return {job_id}

    Note over C, API: 2λ‹¨κ³„: μ‹¤μ‹κ°„ μ΄λ²¤νΈ κµ¬λ…
    C->>API: GET /stream-events/{job_id}
    API->>R: Subscribe (Pub/Sub)

    Note over W, L: 3λ‹¨κ³„: λ°±κ·ΈλΌμ΄λ“ μ²λ¦¬
    W->>W: STT Processing (Whisper)
    loop Every Segment
        W->>R: Publish "transcript_segment"
        R-->>API: Event Message
        API-->>C: SSE Send (Segment Text)
    end

    W->>L: Request Summary (Full Text)
    L-->>W: Return JSON Summary
    W->>R: Publish "final_summary"
    R-->>API: Event Message
    API-->>C: SSE Send (Final Summary)
```



---------------------


## π€ μ‹¤μ‹κ°„ μ¤νΈλ¦¬λ° (WebSocket) μ•„ν‚¤ν…μ² νλ¦„

`test_real_audio_stream.py` μ‹¤ν–‰ μ‹, ν΄λΌμ΄μ–ΈνΈ-μ„λ²„-μ„λΉ„μ¤ κ°„μ μƒμ„Έν• μƒνΈμ‘μ© νλ¦„μ…λ‹λ‹¤.

```mermaid
sequenceDiagram
    participant C as Client
    participant API as API (WebSocket)
    participant VAD as VAD Processor
    participant STT as Whisper Service

    C->>API: Connect WebSocket
    
    loop Audio Stream (30ms Chunk)
        C->>API: Send Binary Chunk
        API->>VAD: Process Chunk
        
        alt Voice Detected (Segment Complete)
            VAD->>STT: Transcribe Segment (Thread)
            STT-->>API: Return Text
            API-->>C: Send JSON {"type": "transcript_segment"}
        end
    end

    C->>API: Disconnect
    API->>STT: Finalize & Summarize
    API-->>C: Send Final Result
```