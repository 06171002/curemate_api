# STT/Summary API

ì‹¤ì‹œê°„ STT(Speech-to-Text) ë° ìš”ì•½ API ì„œë²„ì…ë‹ˆë‹¤.

ì´ í”„ë¡œì íŠ¸ëŠ” `FastAPI`, `Celery`, `Redis`, `faster-whisper`ë¥¼ Docker Composeë¡œ ì‹¤í–‰í•˜ê³ , `Ollama` ë˜ëŠ” `LM Studio`(LLM ì„œë²„)ëŠ” ë¡œì»¬ í˜¸ìŠ¤íŠ¸(Host) PCì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. ì‚¬ì „ ì¤€ë¹„ (ì´ 3ê°€ì§€)

1.  **Docker Desktop**ì„ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
2.  **LLM ì„œë²„ ì¤€ë¹„ (íƒ 1)**
    * ì´ í”„ë¡œì íŠ¸ëŠ” ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. **Ollama** ë˜ëŠ” **LM Studio** ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì„¤ì¹˜í•˜ì„¸ìš”.

    #### [ì˜µì…˜ A] Ollama ì‚¬ìš© ì‹œ
    1.  **Ollama**ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
    2.  ì‚¬ìš©í•  ëª¨ë¸(`gemma3` ë“±)ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
        ```bash
        ollama pull gemma3
        ```

    #### [ì˜µì…˜ B] LM Studio ì‚¬ìš© ì‹œ
    1.  **LM Studio**ë¥¼ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    2.  ì›í•˜ëŠ” ëª¨ë¸(ì˜ˆ: `gemma-2-9b-it`, `llama-3-8b-instruct`)ì„ ê²€ìƒ‰í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    3.  **Local Server** íƒ­(ì¢Œì¸¡ `<->` ì•„ì´ì½˜)ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    4.  ìƒë‹¨ ì¤‘ì•™ì˜ ëª¨ë¸ ì„ íƒ ì°½ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì„ ë¡œë“œ(Load)í•©ë‹ˆë‹¤.
    5.  ìš°ì¸¡ ì„¤ì • íŒ¨ë„ì—ì„œ **Server Port**ê°€ `1234`ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    6.  **"Start Server"** ë²„íŠ¼ì„ ëˆŒëŸ¬ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.
3. ì‹¤í–‰ ì„¤ì • (.env)

í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ `.env` íŒŒì¼ì„ ìˆ˜ì •í•˜ì—¬ ì‚¬ìš©í•  LLMì„ ê²°ì •í•©ë‹ˆë‹¤. (íŒŒì¼ì´ ì—†ë‹¤ë©´ ìƒì„±í•˜ì„¸ìš”)

```ini
# --- LLM ì„ íƒ (ollama ë˜ëŠ” lmstudio) ---
LLM_PROVIDER=lmstudio

# --- LM Studio ì„¤ì • (ê¸°ë³¸ê°’) ---
# Docker ì»¨í…Œì´ë„ˆì—ì„œ í˜¸ìŠ¤íŠ¸ì˜ LM Studioë¡œ ì ‘ì†í•˜ê¸° ìœ„í•œ ì£¼ì†Œ
LMSTUDIO_BASE_URL=[http://host.docker.internal:1234/v1](http://host.docker.internal:1234/v1)

# --- Ollama ì„¤ì • ---
OLLAMA_BASE_URL=[http://host.docker.internal:11434](http://host.docker.internal:11434)
OLLAMA_MODEL_NAME=gemma3
```
### 2. í”„ë¡œì íŠ¸ í´ë¡ 

### 3. ì‹¤í–‰

1.  **[í„°ë¯¸ë„ 1]** ë¡œì»¬ PC(Windows)ì—ì„œ `Ollama` ì„œë²„ë¥¼ **0.0.0.0 í˜¸ìŠ¤íŠ¸**ë¡œ ì‹¤í–‰ (`$env:OLLAMA_HOST="0.0.0.0"`, `ollama serve`)í•˜ê³  ë°©í™”ë²½ì„ í—ˆìš©í•©ë‹ˆë‹¤.
2.  **[í„°ë¯¸ë„ 2]** `docker-compose up -d --build`ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

----------------------------

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```bash
.
â”œâ”€â”€ docker-compose.yml             # GPU ëª¨ë“œ ì‹¤í–‰ ì„¤ì •
â”œâ”€â”€ docker-compose.cpu.yml         # CPU ëª¨ë“œ ì‹¤í–‰ ì„¤ì •
â”œâ”€â”€ run-gpu.sh / run-cpu.sh        # ê°„í¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt               # ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ëª©ë¡
â”‚
â””â”€â”€ stt_api/                       # ğŸ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ íŒ¨í‚¤ì§€
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ main.py                    # FastAPI ì§„ì…ì  (Lifespan, ë¯¸ë“¤ì›¨ì–´, ë¼ìš°í„° ì„¤ì •)
    â”‚
    â”œâ”€â”€ api/                       # ğŸŒ API ì—”ë“œí¬ì¸íŠ¸
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ batch_endpoints.py     # íŒŒì¼ ì—…ë¡œë“œ ë°°ì¹˜ ì²˜ë¦¬ (POST /request, SSE)
    â”‚   â”œâ”€â”€ stream_endpoints.py    # ì‹¤ì‹œê°„ WebSocket ìŠ¤íŠ¸ë¦¬ë° (Google / Faster-Whisper)
    â”‚   â””â”€â”€ stream_endpoints_whisperlive.py # WhisperLiveKit ì „ìš© ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸
    â”‚
    â”œâ”€â”€ core/                      # âš™ï¸ í•µì‹¬ ì„¤ì • ë° ì¸í”„ë¼
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ config.py              # í™˜ê²½ ë³€ìˆ˜ ë° ìƒìˆ˜ ê´€ë¦¬ (Settings)
    â”‚   â”œâ”€â”€ celery_config.py       # Celery ë¹„ë™ê¸° í ì„¤ì • (Redis)
    â”‚   â”œâ”€â”€ database.py            # DB ì—”ì§„ ë° ì„¸ì…˜ ê´€ë¦¬ (SQLAlchemy Async)
    â”‚   â”œâ”€â”€ logging_config.py      # êµ¬ì¡°í™”ëœ ë¡œê¹… (JSON/Color Formatter)
    â”‚   â””â”€â”€ exceptions.py          # ì»¤ìŠ¤í…€ ì˜ˆì™¸ í´ë˜ìŠ¤ ì •ì˜
    â”‚
    â”œâ”€â”€ domain/                    # ğŸ“¦ ë„ë©”ì¸ ëª¨ë¸
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ streaming_job.py       # ìŠ¤íŠ¸ë¦¬ë° ì‘ì—… ìƒíƒœ ë° ë²„í¼ ê´€ë¦¬ ê°ì²´
    â”‚
    â”œâ”€â”€ models/                    # ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ (ORM)
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ database_models.py     # MariaDB í…Œì´ë¸” ë§¤í•‘ (STTJob, STTSegment, STTRoom)
    â”‚
    â””â”€â”€ services/                  # ğŸ› ï¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì„œë¹„ìŠ¤
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ tasks.py               # Celery ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… (ë°°ì¹˜/ë°© ìš”ì•½)
        â”œâ”€â”€ audio_converter.py     # ì˜¤ë””ì˜¤ ë¦¬ìƒ˜í”Œë§ ë° í¬ë§· ë³€í™˜ ìœ í‹¸ë¦¬í‹°
        â”‚
        â”œâ”€â”€ stt/                   # ğŸ¤ STT ì—”ì§„ ëª¨ë“ˆ
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ stt_factory.py     # STT ì—”ì§„ ì„ íƒ íŒ©í† ë¦¬ (Google vs Local)
        â”‚   â”œâ”€â”€ whisper_service.py # Faster-Whisper êµ¬í˜„ì²´ (VAD í¬í•¨)
        â”‚   â”œâ”€â”€ whisperlive_service.py # WhisperLiveKit êµ¬í˜„ì²´ (ì‹¤ì‹œê°„ì„± ê°•í™”)
        â”‚   â”œâ”€â”€ google_stt_service.py  # Google Cloud STT êµ¬í˜„ì²´
        â”‚   â””â”€â”€ vad_processor.py   # ìŒì„± í™œë™ ê°ì§€ (Silero VAD)
        â”‚
        â”œâ”€â”€ llm/                   # ğŸ§  LLM ìš”ì•½ ì„œë¹„ìŠ¤
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ base_llm_service.py # LLM ì¸í„°í˜ì´ìŠ¤ (ì¶”ìƒ í´ë˜ìŠ¤)
        â”‚   â”œâ”€â”€ gemini_service.py   # Google Gemini êµ¬í˜„ì²´
        â”‚   â”œâ”€â”€ ollama_service.py   # Ollama (Local) êµ¬í˜„ì²´
        â”‚   â””â”€â”€ lm_service.py       # LM Studio êµ¬í˜„ì²´
        â”‚
        â”œâ”€â”€ storage/               # ğŸ’¾ ë°ì´í„° ì €ì¥ ë° ê´€ë¦¬
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ job_manager.py     # ì‘ì—… ìƒëª…ì£¼ê¸° í†µí•© ê´€ë¦¬ (DB + Redis íŒŒì‚¬ë“œ)
        â”‚   â”œâ”€â”€ database_service.py # DB CRUD ë¡œì§
        â”‚   â””â”€â”€ cache_service.py   # Redis ìºì‹œ ë° Pub/Sub ë©”ì‹œì§•
        â”‚
        â””â”€â”€ pipeline/              # ğŸ”„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ batch_pipeline.py  # ë°°ì¹˜ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° (íŒŒì¼ -> STT -> ìš”ì•½)
            â””â”€â”€ stream_pipeline.py # ìŠ¤íŠ¸ë¦¬ë° ì›Œí¬í”Œë¡œìš° (ì²­í¬ -> VAD -> STT -> í)
```


---------------------

## ğŸ’¾ íŒŒì¼ í›„ì²˜ë¦¬ (SSE) ì•„í‚¤í…ì²˜ íë¦„

`POST /api/v1/conversation/request` (íŒŒì¼ ì—…ë¡œë“œ)ì™€ `GET /api/v1/conversation/stream-events/{job_id}` (SSE ìŠ¤íŠ¸ë¦¼) ìš”ì²­ ì‹œì˜ ìƒì„¸ íë¦„ì…ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant C as Client (App)
    participant API as API Server
    participant DB as MariaDB
    participant R as Redis (Pub/Sub)
    participant W as Celery Worker
    participant L as LLM (Ollama/Gemini)

    Note over C, W: 1ë‹¨ê³„: ì‘ì—… ìš”ì²­
    C->>API: POST /request (Audio File)
    API->>DB: Create Job (PENDING)
    API->>R: Cache Job Info
    API->>W: Task Queueing (Celery)
    API-->>C: Return {job_id}

    Note over C, W: 2ë‹¨ê³„: ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ êµ¬ë… (ì•ˆì •ì„± ê°•í™”)
    C->>API: GET /stream-events/{job_id}
    API->>DB: Fetch Past Segments
    API-->>C: Send Historical Data (If any)
    API->>R: Subscribe (Pub/Sub)

    Note over W, L: 3ë‹¨ê³„: ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
    W->>W: STT Processing
    loop Every Segment
        W->>DB: Save Segment
        W->>R: Publish "transcript_segment"
        R-->>API: Event Message
        API-->>C: SSE Send (Real-time)
    end

    W->>L: Request Summary (Full Transcript)
    L-->>W: Return JSON Summary
    W->>DB: Update Job (COMPLETED, Summary)
    W->>R: Publish "final_summary"
    R-->>API: Event Message
    API-->>C: SSE Send (Final Summary)
```



---------------------


## ğŸš€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (WebSocket) ì•„í‚¤í…ì²˜ íë¦„

`test_real_audio_stream.py` ì‹¤í–‰ ì‹œ, í´ë¼ì´ì–¸íŠ¸-ì„œë²„-ì„œë¹„ìŠ¤ ê°„ì˜ ìƒì„¸í•œ ìƒí˜¸ì‘ìš© íë¦„ì…ë‹ˆë‹¤.

```mermaid
sequenceDiagram
    participant C as Client (WebRTC)
    participant API as API Server (WS)
    participant AC as AudioConverter
    participant P as StreamPipeline
    participant STT as Whisper/Google
    participant DB as MariaDB

    Note over C, API: 1ë‹¨ê³„: ì—°ê²° ë° ìŠ¤íŠ¸ë¦¬ë°
    C->>API: WebSocket Connect
    API-->>C: Connection Success

    loop Audio Stream (Opus/WebM/PCM)
        C->>API: Send Binary Chunk
        API->>AC: Convert to 16kHz PCM
        AC->>P: 30ms Frame
        
        P->>P: VAD Check (Silero)
        
        alt Voice Detected
            P->>STT: Transcribe (Worker Thread)
            STT-->>P: Text Segment
            P->>DB: Save Segment (Insert)
            P-->>API: Yield Result
            API-->>C: Send JSON {"type": "transcript_segment"}
        end
    end

    Note over C, DB: 2ë‹¨ê³„: ì¢…ë£Œ ë° ìš”ì•½
    C->>API: Disconnect / End
    API->>P: Finalize (Flush Buffer)
    P->>STT: Transcribe Remaining
    
    P->>P: Generate Summary (LLM)
    P->>DB: Update Job (COMPLETED, Summary)
    API-->>C: Send Final Result {"type": "final_summary"}
```

```mermaid
sequenceDiagram
    participant C as Client (WebRTC)
    participant Main as Main Thread<br/>(Event Loop)
    participant Q as Internal Queue
    participant Worker as Worker Thread<br/>(ThreadPool)
    participant DB as MariaDB

    Note over C, Main: âš¡ Main Thread: Non-blocking I/O
    
    loop Audio Stream (Real-time)
        C->>Main: Send Audio Chunk
        activate Main
        Main->>Main: 1. Convert Audio (PCM)
        Main->>Main: 2. Check VAD (Silero)
        
        alt Voice Detected
            Main->>Q: Enqueue Segment
            Note right of Q: (Main Thread continues receiving next audio without waiting)
        end
        
        Main->>Main: 3. Check Result Queue
        
        alt STT Result Ready
            Main->>DB: Save Segment (Insert)
            Main-->>C: Send JSON Result
        end
        deactivate Main
    end

    par Background Processing
        loop STT Worker
            Q->>Worker: Dequeue Segment
            activate Worker
            Note over Worker: ğŸ¢ Heavy CPU Task
            Worker->>Worker: Whisper Inference
            Worker-->>Main: Return Text (via Result Queue)
            deactivate Worker
        end
    end
```